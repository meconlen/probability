\documentclass[11pt, oneside]{book}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{mathabx}
\usepackage{makeidx}
\usepackage[symbol,perpage]{footmisc}
\usepackage[pdftex,bookmarks=true,pdfborder={0 0 0},colorlinks=true,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{setspace}

\newtheorem{definition}{Definition}[chapter]
\newtheorem{example}{Example}[chapter]
\newtheorem{remark}{Remark}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{problem}{Problem}[chapter]

\newcommand{\paren}[1]{\left(#1\right)}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\var}[1]{\mathrm{Var}\left(#1\right)}
\newcommand{\cov}[1]{\mathrm{Cov}\left(#1\right)}



\makeindex

\title{Probability Notes}
\author{Michael Conlen}
%\date{}							% Activate to display a given date or no date

\begin{document}
\frontmatter
\maketitle
\tableofcontents
\setcounter{tocdepth}{4}

\mainmatter
\begin{spacing}{1.618}
\chapter{Introduction}
\section{Sample Spaces and Events}
\begin{definition}[Sample Space]\index{sample space}
	The set of all possible outcomes of an experiment. 
\end{definition}

\begin{example}
	For a coin flip the \emph{sample space} is 
	\begin{alignat}{4}
		S&=\{H, T\}
	\end{alignat}
\end{example}
\begin{example}
	For a $1D6$ die roll the \emph{sample space} is
	\begin{alignat}{4}
		S&=\{1, 2, 3, 4, 5, 6\}
	\end{alignat}
\end{example}

\begin{definition}[Event]\index{event}
	Any subset $E$ of a sample space $S$.
\end{definition}

\begin{remark}
	For any two events $E$ and $F$ of a sample space $S$ we define a new event $E\cup F$ to consist of all outcomes that are either in $E$ or $F$; that is
	\begin{alignat}{4}
		E\cup F&=\{x\in S\mid x\in E \vee x\in F\}
	\end{alignat}
\end{remark}

\begin{remark}
	For any two events $E$ and $F$ of a sample space $S$ we define a new event $EF$, sometimes written $E\cap F$ and referred to as the intersection of $E$ and $F$, as all outcomes which are both in $E$ and $F$; that is, 
	\begin{alignat}{4}
		EF&=\{x\in S\mid x\in E \wedge x\in F\}
	\end{alignat}
\end{remark}

\begin{definition}[Null Event]\index{event!null}\index{null event}
	For any sample space $S$ the null event is an event with no outcomes and is denoted $\emptyset$.
\end{definition}

\begin{example}
	Let $S=\{H, T\}$ and let $E=\{H\}$ and $F=\{T\}$ be events; then the intersection $EF=\emptyset$ is a null event. 
\end{example}

\begin{remark}
	We define the union and intersection of multiple events, $\cup_{n=1}^\infty E_n$ and $\cap_{n=1}^\infty E_n$ as expected. 
\end{remark}

\begin{definition}[Complement]\index{complement}
	Let $E$ be an event in a sample space $S$, then the complement of $E$, denoted $E^C$ is 
	\begin{alignat}{4}
		E^C&=\{x\in S\mid x\not\in E\}
	\end{alignat}
\end{definition}

\section{Probabilities Defined on Events}

\begin{definition}[Probability]\index{probability}
	Let $S$ be a sample space; let $P$ be a function on $\mathcal{P}(S)$ such that for all $E\in\mathcal{P}(S)$ the function $P$ satisfies
	\begin{enumerate}
		\item $0\leq P(E)\leq 1$
		\item $P(S)=1$
		\item For any sequence of events $\{E_n\}$ which are mutually exclusive 
			\begin{alignat}{4}
				P\left(\bigcup_{n=1}^\infty E_n\right)=\sum_{n=1}^\infty P(E_n)
			\end{alignat}
	\end{enumerate}
	We refer to $P(E)$ as the probability of the event $E$. 
\end{definition}

\begin{remark}
	Note that since, for some sample space $S$ and event $E$, that $S=E\cup E^C$ 
	\begin{alignat}{4}
		1&=P(S) \\
			&=P\left(E\cup E^C\right) \\
			&=P(E)+P\left(E^C\right)
	\end{alignat}
	this implies that
	\begin{alignat}{4}
		P\left(E^C\right)=1-P(E)
	\end{alignat}
\end{remark}

\begin{remark}
	The previous remark implies that $P(\emptyset)=P\left(S^C\right)=1-1=0$
\end{remark}

\begin{remark}
	Let $E$ and $F$ be events in a sample space $S$. We wish to compute $P(E\cup F)$. Consider $P(E)$ and $P(F)$, for any $x\in EF$ we find that $x$ is counted once in each of $P(E)$ and $P(F)$; that is
	\begin{alignat}{4}
		P(E)+P(F)=P(E\cup F)-P(EF)
	\end{alignat}
	therefore 
	\begin{alignat}{4}
		P(E\cup F)=P(E)+P(F)-P(EF)
	\end{alignat}

\end{remark}

\begin{lemma}
	Intersection is right (and left) distributive over unions. 
	\begin{proof}
		Let $E$, $F$, $G$ be events in a sample space $S$; we wish to show
		\begin{alignat}{4}
			(E\cup F)G=EG\cup FG
		\end{alignat}
		
		Let $x\in (E\cup F)G$ then $x\in E\cup F$ and $x\in G$. WLOG let $e\in F$; thus $x\in EG\implies x\in G\cup FG$. 
		
		Conversely let $x\in EG\cup FG$. WLOG say $x\in EG$; then $x\in E\implies x\in E\cup F$ and $x\in G$ thus $x\in (E\cup F)G$. 
	\end{proof}
\end{lemma}

\begin{theorem}[Inclusion Exclusion]\index{inclusion exclusion}
	Let $S$ be a sample space and let $E$, $F$, $G$ be events, then 
	\begin{alignat}{4}
		P(E\cup F\cup G)=P(E)+P(F)+P(G)-P(EF)-P(EG)+P(EFG)
	\end{alignat}
	\begin{proof}
		We compute
		\begin{alignat*}{4}
			P(E\cup F \cup G)&=P((E\cup F)\cup G) \\
				&=P(E\cup F)+P(G)-P((E\cup F)G) \\
				&=P(E)+P(F)-P(EF)+P(G)-P((E\cup F)G) \\
				&=P(E)+P(F)-P(EF)+P(G)-P((EG\cup FG)) \\
				&=P(E)+P(F)-P(EF)+P(G)-\left[P(EG) + P(FG) - P(EGFG)\right] \\
				&=P(E)+P(F)-P(EF)+P(G)-\left[P(EG) + P(FG) - P(EFG)\right] \\
				&=P(E)+P(F)-P(EF)+P(G)-P(EG)-P(FG)+P(EFG) \\
				&=P(E)+P(F)+P(G)-P(EF)-P(EG)-P(FG)+P(EFG)
		\end{alignat*}
	\end{proof}
\end{theorem}

\begin{remark}
	Note that the inclusion exclusion principal can be extended to arbitrary unions by adding the intersections of odd combinations of sets and subtracting intersections of even combinations of sets. 
\end{remark}

\section{Conditional Probabilities}
\begin{definition}[Conditional Probability]\index{probablity!conditional}\index{conditional probability}
	Let $S$ be a sample space and let $E$ and $F$ be events. We wish to denote the probability that event $E$ occurred given that $F$ occurred; we write $P(E\mid F)$.
\end{definition}

\begin{remark}
	We can compute $P(E\mid F)$ by realizing that since $F$ has occurred we can consider the event $EF$ as all those containing $E$ and $F$ and limit our sample space to $F$; thus
	\begin{alignat}{4}
		P(E\mid F)=\frac{P(EF)}{P(F)}
	\end{alignat} 
\end{remark}

\section{Independent Events}

\begin{definition}[Independent Events]\index{independent events}\index{event!independent}
	Two events $E$ and $F$ of a sample space $S$ are said to be independent if $P(EF)=P(E)P(F)$
\end{definition}

\begin{definition}[Independent Trials]\index{independent trials}
	Suppose that a sequence of experiments which result in success or failure is to be performed. Let $E_i$ denote the event that the $i^{th}$ experiment is a success. If for all $i_k$
	\begin{alignat}{4}
		P(E_{i_1}E_{i_2}\dots E_{i_n})=\prod_{j=1}^nP(E_{i_j}) 
	\end{alignat}
	then the sequence of experiments consists of independent trials. 
\end{definition}

\section{Bayes' Formula}

\begin{remark}[Bayes' Formula]\index{Bayes' formula}
	Let $S$ be a sample space and let $E$, $F$ be events, then notice that $E=EF \cup EF^C$; that is, some point in $E$ must either be in $F$ or not in $F$ which themselves are mutually exclusive. We can then compute identities 
	\begin{alignat}{4}
		E&=P(EF)+P\left(EF^C\right) \\
			&=P(E\mid F)P(F)+P\left(E\mid F^C\right)P\left(F^C\right) \\
			&=P(E\mid F)P(F)+P\left(E\mid F^C\right)\left(1-P(F)\right)
	\end{alignat}
	If we let $\left\{F_j\right\}$ be a partition of $S$ then 
	\begin{alignat}{4}
		P(E)&=\sum_{i=1}^nP(EF_i) \\
			&=\sum_{i=1}^nP(E\mid F_i)P(F_i)
	\end{alignat} 
	
	Suppose now we assume that $E$ has occurred and we wish to know the probability of each $F_i$ occurring we have 
	\begin{alignat}{4}
		P(F_i\mid E)&=\frac{P(EF_i)}{P(E)} \\
			&=\frac{P(E\mid F_i)P(F_i)}{\sum_{k=1}^nP(E\mid F_k)P(F_k)}
	\end{alignat}
	which is known as Bayes' Formula. 
\end{remark}

\section{Problems}

\begin{problem}[\S 1 \# 5]
	An individual uses the following gambling system at Las Vegas. He bets \$1 that the roulette wheel will come up red. If he wins, he quits. If he loses then he makes the same bet a second time only this time he bets \$2; and then regardless of the outcome he quits. Assuming that he has a probability of $\frac{1}{2}$ of winning each bet, what is the probability that he goes home a winner. Why is this system not used by everyone? 
	\begin{proof}[Solution]
		Suppose he wins the first bet, he quits a winner winning \$1. Suppose he does not win the first bet, then his second bet gives him $\frac{1}{2}$ odds of winning \$2 on the bet covering the \$1 loss plus \$1 winnings. He has the same odds for losing; therefore he wins $\frac{3}{4}$ of the time. His expected value is 
		\begin{alignat*}{4}
			\frac{1}{2}\cdot 1 + \frac{1}{4}\cdot (-3) + \frac{1}{4}\cdot (1) &= \frac{1}{2} - \frac{2}{4} + \frac{1}{4} \\
				&=\frac{1}{2} - \frac{3}{4} + \frac{1}{4} \\
				&=0
		\end{alignat*}
		The reason this isn't used is that the odds of red (or black) are actually less than $\frac{1}{2}$. In the US the odds of red (or black) are $\frac{18}{40}$; so now we compute the expected value
		\begin{alignat*}{4}
			\frac{18}{40}\cdot 1 + \frac{22}{40}\cdot\frac{22}{40}\cdot (-3) + \frac{22}{40}\cdot\frac{18}{40} \cdot 1 &=\frac{18}{40}-\frac{968}{1600} + \frac{396}{1600} \\
				&=\frac{720}{1600} - \frac{1452}{1600} + \frac{396}{1600} \\
				&=\frac{-336}{1600} \\
				&=-\frac{21}{100}
		\end{alignat*}
	\end{proof}
\end{problem}

\begin{problem}[\S 1 \# 9]
	We say that $E \subset F$ if every point in $E$ is also in $F$. Show that $E\subset F$ implies 
	\begin{alignat}{4}
		P(F)=P(E) + P\left(FE^C\right) \geq P(E)
	\end{alignat}
	\begin{proof}[Solution]
		We compute each equality separately. 
		
		Recall that $F=FE \cup FE^C$ and that since $E$ and $E^C$ are disjoint we can write $P(F)=P(FE) + P\left(FE^C\right)$ but since $E\subset F$ we find that $FE = E$ and so $P(FE)=P(E)$; so $P(F)=P(E)+P\left(FE^C\right)$ is established. 
		
		Now, since probabilities are non-negative the inequality on the right holds since $P\left(FE^C\right)$ may be non-negative including $0$ in the case where $E=F$ since in that case $FE^C=\emptyset$. 
		
	\end{proof}
\end{problem}

\begin{problem}[\S 1 \# 17]
	Suppose each of three persons tosses a coin. If the outcome of one of the tosses differs from the other outcomes, then the game ends. If not, the persons start over and retoss their coins. Assuming fair coins, what is the probability that the game will end with the first route of tosses? If all three coins are biased with probability of $\frac{1}{4}$ of landing heads, what is the probability that the game will end at the first route? 
	\begin{proof}[Solution]
		In the first case we consider each of three independent events. Given the first independent event is heads (WLOG) then there is a $\frac{1}{2}$ that the second event is heads and the same for the third so the probability that the game continues after the first toss is $\frac{1}{4}$; therefore the probability that the game ends is $1-\frac{1}{4}=\frac{3}{4}$. 
		
		In the second case we have $\frac{1}{4}$ that the first event is heads, followed by the same for the other two events so the probability that the game ends on the first toss with a heads result is $\frac{1}{4^3}=\frac{1}{64}$. Conversely if the first event is tails with probability $\frac{3}{4}$ then we have the same for the second and third events with an ultimate probability of $\frac{27}{64}$ thus the total probability of the game continuing is $\frac{28}{64}=\frac{7}{16}$ and the probability that the game ends is $1-\frac{7}{16}=\frac{9}{16}$. 
	\end{proof}
\end{problem}

\begin{problem}[\S 1 \# 19]
	Two dice are throw. What is the probability that at least one is a six? If the two faces are different, what is the probability that at least one is a six? 
	\begin{proof}[Solution]
		In the first case we have $\frac{1}{6}$ that the first face is a six. In the $\frac{5}{6}$ that it is not we also have a $\frac{1}{6}$ chance that the second face is a six, so the probability is $\frac{1}{6}+\frac{5}{36}=\frac{11}{36}$ 
		
		In the second case we still have $\frac{1}{6}$ probability that the first face is a six, but now in the $\frac{5}{6}$ we have a $\frac{1}{5}$ chance that the second face is a six since we know it's face is not the non-six that the first face had; thus the probability is
		\begin{alignat*}{4}
			\frac{1}{6}+\frac{5}{6}\frac{1}{5}&= \frac{5}{30}+\frac{5}{30} \\
				&=\frac{10}{30} \\
				&=\frac{1}{3}
		\end{alignat*}
		
		We can compute the second case by conditional probabilities. Let $E$ be the event that at least one face is a six and $F$ is the event that the faces are different, we want to compute $P(E\mid F)$. We know that $P\left(F^C\right)$, the probability of the event that they are the same is $\frac{1}{6}$ since for each face for the first die there is precisely one six events for the second die to match; therefore $P(F)=\frac{5}{6}$. Now we compute $P(EF)$ which is the probability that one face is a six and that each face is different, this is the 5 ways the first die can be a six and the other die different and likewise the 5 ways the second can be 6 when the first is different, so $P(EF)=\frac{10}{36}$.
		\begin{alignat*}{4}
			P(E\mid F)&=\frac{P(EF)}{P(F)} \\
				&=\frac{\frac{10}{36}}{\frac{5}{6}} \\
				&=\frac{10}{36}\frac{6}{5} \\
				&=\frac{1}{3}
		\end{alignat*} 
	\end{proof}
\end{problem}

\begin{problem}[\S 1 \# 25]
	Two cards are randomly selected from a desk of 52 playing cards. 
	\begin{enumerate}
		\item What is the probability that they constitute a pair? 
		\item What is the probability that they constitute a pair given that they are from different suits? 
	\end{enumerate}
	\begin{proof}[Solution]
		Given that we select some card in the first event there are precisely 3 cards of the remaining 51 cards which will make a pair, so the probability is $\frac{3}{51}$
		
		Given that we select some card in the first event we want to compute $P(E\mid F)$ where $E$ is the event that we select a pair card and $F$ is the event that we select a card of a different suit; we wish to compute $\frac{P(EF)}{P(F)}$. The event $EF$ is the event that we select a pair card and it's of a different suit, but $E$ implies $F$ so $P(EF)=\frac{3}{51}$. The even $F$ is the event that we select a card of a different suit. There are $39$ cards of a different suit so $P(F)=\frac{39}{51}$ and we compute
		\begin{alignat*}{4}
			P(E\mid F)&=\frac{P(EF)}{P(F)} \\
				&=\frac{\frac{3}{51}}{\frac{39}{51}} \\
				&=\frac{3}{39} \\
				&=\frac{1}{13}
		\end{alignat*}
	\end{proof}
\end{problem}

\begin{problem}[\S 1 \#23]\label{P1.23}
	For events $E_1, E_2, \dots$ show that 
	\begin{alignat}{4}
		P(E_1E_2\dots E_n)=P(E_1)P(E_2\mid E_1)P(E_3\mid E_1E_2)\dots P(E_n\mid E_1\dots E_{n-1})
	\end{alignat}
	\begin{proof}
		We prove by induction on $n$. Let $n=2$, then the result follows from the general formula for conditional probabilities; that is
		\begin{alignat*}{4}
			P(E_1E_2)&=P(E_1)P(E_2\mid E_1)
		\end{alignat*}
		Note that $P(E_1E_2)=P(E_2E_1)$ since the intersection operation is commutative. Now, let $n$ be given and let 
		\begin{alignat*}{4}
			P(E_1E_2\dots E_{n-1})=P(E_1)P(E_2\mid E_1)P(E_3\mid E_1E_2)\dots P(E_{n-1}\mid E_1\dots E_{n-2})
		\end{alignat*} 
		be given by inductive hypothesis. We may write $F_1=E_1E_2\dots E_{n-1}$ and $P(F_1)=P(E_1)P(E_2\mid E_1)P(E_3\mid E_1E_2)\dots P(E_{n-1}\mid E_1\dots E_{n-2})$ and let $F_2=E_n$ such that 
		\begin{alignat*}{4}
			P(E_1\dots E_n)&=P(F_1F_2) \\
				&=P(F_1)P(F_2\mid F_1) \\
				&=P(E_1)P(E_2\mid E_1)P(E_3\mid E_1E_2)\dots P(E_{n-1}\mid E_1\dots E_{n-2})P(F_2\mid F_1) \\
				&=P(E_1)P(E_2\mid E_1)P(E_3\mid E_1E_2)\dots P(E_{n-1}\mid E_1\dots E_{n-2})P(F_2\mid E_1E_2\dots E_{n-1}) \\
				&=P(E_1)P(E_2\mid E_1)P(E_3\mid E_1E_2)\dots P(E_n\mid E_1E_2\dots E_{n-1}) \\	
		\end{alignat*}
		as required. 
	\end{proof}
\end{problem}

\begin{problem}[\S 1 \# 27]
	A deck of 52 playing cards, containing all four aces, is randomly divided into 4 piles of 13 cards each. Define events $E_1$, $E_2$, $E_3$, $E_4$ as follows
	\begin{enumerate}
		\item $E_1$ = One of the piles contains the ace of spades.
		\item $E_2$ = The ace of spades and the ace of hearts are in different piles.
		\item $E_3$ = The ace of spades, the ace of hearts and the ace of diamonds are in different piles. 
		\item $E_4$ = All four aces are in different piles.
	\end{enumerate}
	Use Problem \ref{P1.23} to compute the probability that each pile has an ace. 
	\begin{proof}[Solution]
		incomplete
	\end{proof}
\end{problem}

\begin{problem}[\S 1 \# 30]
	Bill and George go target shooting together. Both shoot at a target at the same time. Suppose Bill hits the target with probability $0.7$, whereas George independently hits the target with probability $0.4$. 
	\begin{enumerate}
		\item Given that exactly one shot hit the target, what is the probability that it was George's shot? 
		\item Give that the target is hit, what is the probability that George hit it? 
	\end{enumerate}
	\begin{proof}[Solution]
		Suppose exactly one shot hit the target. The probability that both hit the target is $0.7\cdot 0.4=0.28$. The probability that neither hit the target is $0.3\cdot 0.6=0.18$. The remaining probability that precisely one person hit the target, call his event $E$, is $0.54$. Let's call the event that George hit the target $G$. We need to know the probability that George hit the target and only one person hit; that is, the probability that George hit and Bill didn't, this is $0.4\cdot 0.3 = 0.12$. 
		\begin{alignat*}{4}
			P(G\mid E) &= \frac{P(EG)}{P(E)} \\
				&=\frac{0.12}{0.54} \\ 
				&=\frac{2}{9}
		\end{alignat*}
		
		Now suppose that the garget is hit, call this event $F$, this is $1$ minus neither hit, $1-0.3\cdot 0.6 = 1-0.18 = 0.82$. To compute $P(GF)$ we consider that $G\subset F$ so that $P(GF)=P(G)$. 
		\begin{alignat*}{4}
			P(G\mid F) &= \frac{P(GF)}{P(F)} \\
				&=\frac{0.4\cdot 0.4}{0.82} \\
				&=\frac{20}{81}
		\end{alignat*}
	\end{proof}
\end{problem}

\begin{problem}[\S 1 \#32]
	Suppose all $n$ men at a party throw their hats in the center of the room. Each man then randomly selects a hat. Show that the probability that none of the men select their own hat is
	\begin{alignat}{4}
		\sum_{i=2}^n(-1)^n\frac{1}{n!}
	\end{alignat}
	\begin{proof}[Solution]
		Let $E_i$ be the event that person $i$ has their own hat. Let event $E=\bigcup E_i$ be the event that some person has their own hat and $\widebar{E}$ be the event that no person has their own hat, $P(\widebar{E})=1-P(E)$. We wish to compute $P(E)$ which we can do using the inclusion exclusion principal. 
		
		The probability that any person picks their own hat is $P(E_i)=\frac{1}{n}$. Now recall that $P(E_iE_j)=P(E_j\mid E_i)P(E_i)$, we need to compute $P(E_j\mid E_i)$. Given that $E_i$ occurred there are $(n-1)$ hats remaining including hat $j$ so $P(E_j\mid E_i)=\frac{1}{n-1}$ so 
		\begin{alignat*}{4}
			P(E_iE_j)&=\frac{1}{n}\frac{1}{n-1} \\
				&=\frac{(n-2)!}{n!}
		\end{alignat*}
		
		Now suppose that $E_{i_1}E_{i_2}\dots E_{i_k}$ has occurred, the probability that $E_{i_1} \dots E_{i_k}E_{i_{k+1}}$ occurs is 
		\begin{alignat*}{4}
			P(E_{i_1}\dots E_{i_{k+1}})&=P(E_{i_{k+1}}\mid E_{i_1}\dots E_{i_k})P(E_{i_1}\dots E_{i_k}) \\
				&=\frac{1}{n-k}\frac{(n-k)!}{n!} \\
				&=\frac{(n-(k+1))!}{n!}
		\end{alignat*}
		
		Now we apply the Inclusion Exclusion principal to compute the probability that some person picks their hat. This is the $\binom{n}{1}$ probabilities of one person picking their hat minus the $\binom{n}{2}$ probabilities of two people picking their hat and so on. 

		\begin{alignat*}{4}
			P(E)&=P\left(\bigcup E_i\right) \\
				&=\binom{n}{1}\frac{1}{n}-\binom{n}{2}\frac{(n-2)!}{n!}+\dots \\
				&=\sum_{k=1}^n (-1)^{k+1}\binom{n}{k}\frac{(n-k)!}{n!} \\
				&=\sum_{k=1}^n(-1)^{k+1} \frac{n!}{(k!)(n-k)!}\frac{(n-k)!}{n!} \\
				&=\sum_{k=1}^n(-1)^{k+1}\frac{1}{k!}
		\end{alignat*}
		so $P\left(\widebar{E}\right)$ is 
		\begin{alignat*}{4}
			P\left(\widebar{E}\right)&=1-P(E) \\
				&=1+(-1)\sum_{k=1}^n(-1)^{k+1}\frac{1}{k!} \\
				&=1+\sum_{k=1}^n(-1)^k\frac{1}{k!} \\
				&=1+(-1)\frac{1}{1!}+\sum_{k=2}^n(-1)^k\frac{1}{k!} \\
				&=\sum_{k=2}^n(-1)^k\frac{1}{k!} \\
				&=\frac{1}{2!}-\frac{1}{3!}+\frac{1}{4!} -+ \dots (-1)^n\frac{1}{n!}
		\end{alignat*}
	\end{proof}
\end{problem}


\begin{problem}[\S 1 \#40]
	A gambler has in his pocket a fair coin and a two-headed coin. He selects one of the coins at random, and when he flips it, it shows heads. What is the probability that it is the fair coin? 
	
	Suppose that he flips the same coin a second time and again it shows heads. Now what is the probability that it is the fair coin? 
	
	Suppose that he flips it a third time and it shows tails. Now what is the probability that it is the fair coin? 
	
	\begin{proof}[Solution]
		Let $F$ be the event that he selects the fair coin, let $H_1$ be the event that the first flip is heads, we wish to compute $P(S\mid H_1)$. 
		\begin{alignat*}{4}
			P(F\mid H_1)&=\frac{P(F H_1)}{P(H_1)}
		\end{alignat*}
		There are three heads and one tails, so $P(H_1)=\frac{3}{4}$ and $P(F H_1)=\frac{1}{4}$ so 
		\begin{alignat*}{4}
			P(F\mid H_1)&=\frac{\frac{1}{4}}{\frac{3}{4}} \\
				&=\frac{1}{3}
		\end{alignat*}
	
		Now suppose that the coin is flipped a second time and shows heads, we want to compute $P(F\mid H_1H_2)$
		\begin{alignat*}{4}
			P(F\mid H_1H_2)&=\frac{P(FH_1H_2)}{P(H_1H_2)}
		\end{alignat*}
		For $P(FH_1H2)$ we have the expected $\frac{1}{4}$ result of getting two heads in a row with a fair coin. Overall for $P(H_1H_2)$ we have the four ways that an unfair coin can return two heads plus the 1 in 4 way that a fair coin can return two heads so $P(H_1H_2)=\frac{5}{4}$ and 
		\begin{alignat*}{4}
			P(F\mid H_1H_2)&=\frac{\frac{1}{4}}{\frac{5}{4}} \\
				&=\frac{1}{5}
		\end{alignat*}
		
		Now suppose that a tails is recorded, the probability of a fair coin is $1$ as the probability of a tails with the unfair coin is $0$. 
	\end{proof}
	
\end{problem}

\begin{problem}[\S 1 \#45]
	An urn contains $b$ black balls and $r$ red balls. One of the balls is drawn at random, but when it is put back in the urn $c$ additional balls of the same color are put in with it. Now suppose that we draw another ball. Show that the probability that the first ball drawn was black given the second ball was red is
	\begin{alignat}{4}
		\frac{b}{b+r+c}
	\end{alignat}
	\begin{proof}[Solution]
		Let's denote the event that the first ball is black with $B_1$ and the second ball is red with $R_2$. We wish to compute 
		\begin{alignat}{4}
			P(B_1\mid R_2)&=\frac{P(B_1R_2)}{P(R_2)}
		\end{alignat}
		
		We compute $P(R_2)$ noting that $P(B_1)=\frac{b}{b+r}$ and $P(R_1)=\frac{r}{b+r}$
		\begin{alignat}{4}
			P(R_2)&=P(B_1)P(R_2\mid B_1)+P(R_1)P(R_2\mid R_1) \\
				&=\left(\frac{b}{b+r}\right)\left(\frac{r}{b+r+c}\right)+\left(\frac{r}{b+r}\right)\left(\frac{r+c}{b+r+c}\right) \\
				&=\left(\frac{(r)(b+r+c)}{(b+r)(b+r+c)}\right) \\
				&=\frac{r}{b+r}
		\end{alignat}
		
		Now to compute $P(B_1R_2)$ we find that $P(B_1)=\frac{b}{b+r}$ and of those $R_2$ happens $\frac{r}{b+r+c}$ so 
		\begin{alignat}{4}
			P(B_1R_2)&=\left(\frac{b}{b+r}\right)\left(\frac{r}{b+r+c}\right) \\
				&=\frac{br}{(b+r)(b+r+c)}
		\end{alignat}
		
		so we compute
		\begin{alignat}{4}
			P(B_1\mid R_2)&=\frac{(B_1R_2)}{P(R_2)} \\
				&=\frac{\frac{br}{(b+r)(b+r+c)}}{\frac{r}{b+r}} \\
				&=\frac{b}{b+r+c}
		\end{alignat}
		as required
	\end{proof}
\end{problem}

\begin{problem}[\S 1 \# 48]
	Sixty percent of families in a certain community own their own car, thirty percent own their own home and twenty percent own both their own car and their own home. If a family is randomly chosen, what is the probability that this family owns a car or a house but not both. 
	\begin{proof}[Solution]
		We denote home ownership with the event $H$ and car ownership with the event $C$ and $HC$ for the event that they own both and $H\cup C$ the event that they own either. Note that $P(H\cup C)=P(H)+P(C)-P(HC)$ and we wish to compute $P(H\cup C)-P(HC)$. 
		\begin{alignat}{4}
			P(H\cup C)-P(HC)&=P(H)+P(C)-P(HC)-P(HC) \\
				&=0.3+0.6-0.2-0.2 \\
				&=0.5
		\end{alignat}
	\end{proof}
\end{problem}

\chapter{Random Variables}

\section{Random Variables}

\begin{definition}[Random Variable]\index{random variable}
	A \emph{random variable} is a real valued function defined on a sample space; that is a random variable $X$ defined on a sample space $S$ is a map
	\begin{alignat}{4}
		X:S\to \mathbb{R}
	\end{alignat}	
\end{definition}

\begin{example}
	If our experiment is to roll two die we may define a random variable to be the sum of the die rolls so that $X( (1, 1) ) = 2$ and $X( (3, 4) )=7$. 
\end{example}

\begin{remark}
	A random variable can be the outcome of an experiment; for example when we roll two die if the random variable $X$ is the sum of the dice then $P\{X=2\}=\frac{1}{36}$ as there's only one way for this to occur, with roll $(1, 1)$. Likewise $P\{X=7\}=\frac{1}{6}$ since of the 36 possible outcomes of the die rolls $6$ of them result in a sum of $7$. 
\end{remark}

\begin{definition}[Indicator Random Variable]\index{random variable!indicator}\index{indicator random variable}
	A random variable which denotes $1$ or $0$ depending on whether or not some event $E$ occurs 
\end{definition}

\begin{definition}[Cumulative Distribution Function]\index{cumulative distribution function!discrete}
	We define a function 
	\begin{alignat}{4}
		F(x)=P\left\{X\leq x\right\}
	\end{alignat}
	that is, $F(x)$ is the probability that $X\leq x$. We call this the cumulative distribution function and often write CDF.  
\end{definition}



\section{Discrete Random Variables}

\begin{definition}[Discrete Random Variable]\index{random variable!discrete}\index{discrete random variable}
	A random variable which can take on at most a countable number of possible values
\end{definition}

\begin{definition}[Probability Mass Function]\index{probability mass function}
	Let $X$ be a discrete random variable then the function 
	\begin{alignat}{4}
		p(a) = P\left\{X=a\right\}
	\end{alignat}
	is called the probability mass function. We often write PMF. 
\end{definition}

\begin{remark}
	A probability mass function $p$ may take on positive values for at most a countable number of values in the domain $\mathbb{R}$\footnote{recall that the domain of the PMF is the range of the random variable whose range is $\mathbb{R}$.}, so we may index these $x_i$ and since the random variable $X$ must take on these values we find
	\begin{alignat}{4}
		\sum_{i=1}^\infty p(x_i)=1
	\end{alignat}
\end{remark}

\begin{remark}
	Discrete random variables are often classified by their probability mass function. 
\end{remark}

\begin{definition}[Cumulative Distribution Function of Discrete Random Variables]\index{cumulative distribution function!discrete}
	Let $X$ be a discrete random variable taking on values $\left\{x_i\right\}_{i\in\mathbb{N}}$; then the cumulative distribution function is given by 
	\begin{alignat}{4}
		F(a)=\sum_{x_i\leq a}p(x_i)
	\end{alignat} 
\end{definition}



\subsection{Bernoulli Random Variable}

\begin{definition}[Bernoulli Random Variable]\index{Bernoulli random variable}\index{random variable!Bernoulli}
	The Bernoulli random variable is an indicator random variable characterized by 
	\begin{alignat}{4}
		p(0) = P\left\{X=0\right\} = 1-p \label{Eqn:2.1} \\
		p(1) = P\left\{X=1\right\} = p \label{Eqn:2.2}
	\end{alignat}
	where $0\leq p\leq 1$. We say that $p$ is the probability of success 
\end{definition}

\begin{remark}
	Suppose we conduct an experiment with probability of success $p$ and probability of failure $1-p$ then the random variable $X$ given by equations \ref{Eqn:2.1} and \ref{Eqn:2.2} is a Bernoulli random variable. 
\end{remark}

\subsection{Binomail Random Variable}
\begin{definition}[Binomial Random Variable]\index{binomial random variable}\index{random variable!binomial}
	Suppose we conduct $n$ identical independent Bernoulli trails with probability of success $p$, if we let $X$ be the random variable that indicates the number of successes that occur in the $n$ trials then $X$ is said to be a binomial random variable with parameters $(n, p)$ and is characterized by the probability mass function
	\begin{alignat}{4}
		p(i)=\binom{n}{i}p^i(1-p)^{n-i}
	\end{alignat}
	for $i\in \left\{0, \dots, n\right\}$. 
\end{definition}

\subsection{The Geometric Random Variable}

\begin{remark}
	Suppose that independent trials, each having probability $p$ of being a success are performed until a success occurs. If we let $X$ be the number of trails required until the first success then $X$ is said to be a geometric random variable with parameter $p$. 
\end{remark}

\begin{definition}[Geometric Random Variable]\index{random variable!geometric}\index{geometric random variable}
	A random variable $X$ with the probability mass function
	\begin{alignat}{4}
		p(n)=P\left\{X=n\right\} = (1-p)^{n-1}p
	\end{alignat}
	for $n\in\mathbb{N}$ is a geometric random variable. 
\end{definition}

\begin{remark}
	We can see that the definition follows the description since for $X=n$ we require $n-1$ failures with probability $1-p$ and a single success with probability $p$. 
\end{remark}

\subsection{The Poisson Random Variable}

\begin{definition}[Poisson Random Variable]\index{random variable!poisson}\index{poison random variable}
	A random variable $X$ taking on values in $\mathbb{N}$ with the probability mass function 
	\begin{alignat}{4}
		p(i)=p\left\{X=i\right\} = e^{-\lambda}\frac{\lambda^i}{i!}
	\end{alignat}
	is said to be a Poisson random variable with parameter $\lambda$
\end{definition}

\begin{remark}
	The Poisson random variable is the limit of binomial random variable with parameters $(n, p)$ when $n\to\infty$ and $n\cdot p=\lambda$ is held fixed; thus for large $n$ and small $p$ the poisson random variable is a decent approximation to a binomial random variable. 
\end{remark}

\section{Continuous Random Variables}

\begin{definition}[Continuous Random Variable]\index{random variable!continuous}\index{continuous random variable}
	Let $X$ be a random variable such that $X$ maps the sample space to an uncountable set in $\mathbb{R}$ such that there exists a function
	\begin{alignat}{4}
		f:\mathbb{R}\to\mathbb{R}^+
	\end{alignat}
	such that for some set $B$ 
	\begin{alignat}{4}
		P\left\{X\in B\right\} = \int_{B}f(x)dx
	\end{alignat}
\end{definition}

\begin{remark}
	Recall that there is an experiment with sample space $S$ and a random variable $X:S\to\mathbb{R}$, $B$ is a subset of the range of $X$. 
\end{remark}

\begin{definition}[Probability Density Function]\index{probability density function}
	If $X$ is a continuous random variable such that 
	\begin{alignat}{4}
		P\left\{X\in B\right\}=\int_Bf(x)dx
	\end{alignat}
	then $f$ is the probability density function. 
\end{definition}

\begin{remark}
	We can completely characterize continuous random variables in terms of the probability density function.
\end{remark}

\begin{definition}[Cumulative Distribution Function of a Continuous Random Variable]\index{cumulative distribution function!continuous}
	The relationship between the cumulative distribution function $F(\cdot)$ and the probability density function $f(\cdot)$ is given by 
	\begin{alignat}{4}
		F(a)=P\left\{X\in (-\infty, a]\right\} = \int_{-\infty}^af(x)dx
	\end{alignat}
	and by the FTC we have
	\begin{alignat}{4}
		\frac{d}{da}F(a)=f(a)
	\end{alignat}
\end{definition}

\subsection{The Uniform Random Variable}

\begin{definition}[Uniform Random Variable]\index{random variable!uniform}\index{random variable}
	A random variable is said to be uniformly distributed over the interval $(\alpha, \beta)$ if the probability density function is given by 
	\begin{alignat}{4}
		f(x)=\left\{\begin{array}{lll}
			\frac{1}{\beta - \alpha},&\hspace{1 cm}&\alpha < x < \beta \\
			0,&\hspace{1 cm}&\textrm{otherwise}
		\end{array}\right.
	\end{alignat}
\end{definition}

\begin{remark}
	The CDF of a uniform random variable is given by 
	\begin{alignat}{4}
		F(a)&=\left\{\begin{array}{lll}
			0,&\hspace{1 cm}&a\leq \alpha \\
			\frac{a - \alpha}{\beta - \alpha},&\hspace{1 cm}& \alpha<a<\beta \\
			1,&\hspace{1 cm}& a \geq \beta 
		\end{array}\right.
	\end{alignat}
\end{remark}

\subsection{Exponential Random Variables}

\begin{definition}\index{random variable!exponential}\index{exponential random variable}
	A continuous random variable whose probability density function is given, for some $\lambda > 0$, by 
	\begin{alignat}{4}
		f(x)&=\left\{\begin{array}{lll}
			\lambda e^{-\lambda x},&\hspace{1 cm}&x\geq 0 \\
			0,&\hspace{1 cm}&x< 0
		\end{array}\right.
	\end{alignat}
\end{definition}

\begin{remark}
	The cumulative distribution function of an exponential random variable with parameter $\lambda$ is 
	\begin{alignat}{4}
		F(a)&=\int_0^a\lambda e^{-\lambda x} dx
	\end{alignat}
\end{remark}

\subsection{Gamma Random Variable}

\begin{definition}[Gamma Function]\index{gamma function}
	The Gamma function is given by 
	\begin{alignat}{4}
		\Gamma(\alpha)&=\int_0^\infty e^{-x}x^{\alpha - 1} dx
	\end{alignat}
\end{definition}

\begin{definition}[Gamma Random Variable]\index{gamma random variable}\index{random variable!gamma}
	A continuous random variable with probability density function, for some $\lambda > 0$ and $\alpha > 0$ 
	
	\begin{alignat}{4}
		f(x)&=\left\{\begin{array}{lll}
			\frac{\lambda e^{-\lambda x} (\lambda x)^{\alpha-1}}{\Gamma(\alpha)},&\hspace{1 cm}& x\geq 0 \\
			0,&\hspace{1 cm}& x< 0
		\end{array}\right.
	\end{alignat}
	is a \emph{gamma random variable} with parameters $\alpha$ and $\lambda$
\end{definition}

\subsection{Normal Random Variables}

\begin{definition}[Normal Random Variable]\index{random variable!normal}\index{normal random variable}
	Let $X$ be a continuous random variable with parameters $\mu$ and $\sigma^2$, then we say that $X$ is a normal random variable if it has the probability density function
	\begin{alignat}{4}
		f(x)&=\frac{1}{\sqrt{2\pi}\sigma}exp\left\{\frac{-(x-\mu)^2}{2\sigma^2}\right\}
	\end{alignat}
	for $x\in\mathbb{R}$. 
\end{definition}

\begin{remark}
	Let $X$ be a normal random variable with parameters $\mu$ and $\sigma^2$; then $Y=\alpha X+\beta$ is a normal random variable with parameters $\alpha\mu+\beta$ and $\alpha^2\sigma^2$. 
\end{remark}

\begin{definition}[Standard Distribution]\index{standard distribution}
	Let $X$ be a normal random variable with parameters $\mu$ and $\sigma^2$ and let $Y=\frac{X-\mu}{\sigma}$ then $Y$ is a normal random variable with parameters $(0, 1)$ and we call this the standard normal distribution. 
\end{definition}

\section{Expectation of a Random Variable}

\subsection{Discrete Random Variables}

\begin{definition}[Expectation of a Discrete Random Variable]\index{expectation!discrete random variable}\index{discrete random variable!expectation}
	Let $X$ be a discrete random variable with probability mass function $p(x)$, then the \emph{expected value} of $X$ is defined by
	\begin{alignat}{4}
		E[X]=\sum_{x:p(x)>0} xp(x)
	\end{alignat}
\end{definition}

\begin{proposition}[Expectation of a Bernoulli Random Variable]\index{expectation!Bernoulli}\index{Bernoulli random variable!expectation}
	Let $X$ be a Bernoulli random variable with parameter $p$, then the expectation of $X$ is given by
	\begin{alignat}{4}
		E[X]=p
	\end{alignat}
\end{proposition}

\begin{proposition}[Expectation of a Binomial Random Variable]\index{expectation!binomial}\index{binomial random variable!expectation}
	Let $X$ be a binomial random variable with parameters $(n, p)$ then the expectation of $X$ is given by 
	\begin{alignat}{4}
		E[X]=np
	\end{alignat}
\end{proposition}

\begin{proposition}[Expectation of a Geometric Random Variable]\index{expectation!geometric}\index{geometric random variable!expectation}
	Let $X$ be a geometric random variable with parameter $p$, then the expectation of $X$ is given by 
	\begin{alignat}{4}
		E[X]=\frac{1}{p}
	\end{alignat}
\end{proposition}

\subsection{Continuous Random Variables}
\begin{definition}[Expectation of a Continuous Random Variable]\index{expectation!continuous random variable}\index{continuous random variable!expectation}
	Let $X$ be a continuous random variable with probability density function $f(x)$, then the \emph{expected value} of $X$ is defined by
	\begin{alignat}{4}
		E[X]&=\int_{-\infty}^\infty xf(x)dx
	\end{alignat}
\end{definition}

\begin{proposition}[Expectation of a Uniform Random Variable]\index{uniform random variable!expectation}\index{expectation!uniform}
	Let $X$ be a continuous uniform random variable distributed over $(\alpha, \beta)$, then the expectation of $X$ is given by 
	\begin{alignat}{4}
		E[X]&=\frac{\beta+\alpha}{2}
	\end{alignat}
\end{proposition}

\begin{proposition}[Expectation of an Exponential Random Variable]\index{exponential random variable!expectation}\index{expectation!exponential}
	Let $X$ be an exponential random variable with parameter $\lambda$, then the expectation of $X$ is given by 
	\begin{alignat}{4}
		E[X]=\frac{1}{\lambda}
	\end{alignat}
\end{proposition}

\begin{proposition}[Expectation of a Normal Random Variable]\index{normal random variable!expectation}\index{expectation!normal}
	Let $X$ be a normal random variable with parameters $(\mu, \sigma^2)$, then the expectation of $X$ is given by 
	\begin{alignat}{4}
		E[X]=\mu
	\end{alignat}
\end{proposition}

\subsection{Expectation of a function of a Random Variable}

\begin{remark}
	Recall that a random variable $X$ is a function 
	\begin{alignat}{4}
		X:S\to \mathbb{R}
	\end{alignat}
\end{remark}

\begin{proposition}
	Let $X$ be a discrete random variable and $g:X\to \mathbb{R}$ then 
	\begin{alignat}{4}
		E[g(X)]=\sum_{x\mid p(x)>0}g(x)p(x)
	\end{alignat}
\end{proposition}

\begin{proposition}
	Let $X$ be a continuous random variable and $g:X\to\mathbb{R}$ then 
	\begin{alignat}{4}
		E[g(X)]=\int_{\mathbb{R}}g(x)f(x)dx
	\end{alignat}
\end{proposition}

\begin{corollary}
	Expectations are linear; that is for constants $a$ and $b$
	\begin{alignat}{4}
		E[aX+b]&=aE[X]+b
	\end{alignat}
\end{corollary}

\begin{definition}[Moment]\index{moment}
	The expected value of a random variable $X$, $E[X]$ is referred to as the \emph{mean}\index{mean} or the first \emph{moment} of $X$. The value $E\left[X^n\right]$ is called the $n^{th}$ moment of $X$. 
\end{definition}

\begin{definition}[Variance]\index{variance}
	The \emph{variance} of a random variable $X$ is given by 
	\begin{alignat}{4}
		\var{X}=E\left[\left(X-E[X]\right)^2\right]
	\end{alignat}
\end{definition}

\section{Jointly Distributed Random Variables}

\subsection{Joint Distribution Functions}

\begin{remark}
	When we think about joint distribution functions we construct the cumulative distribution function and derive from there
\end{remark}

\begin{definition}[Joint Cumulative Distribution Function]\index{cumulative distribution function!joint}\index{joint cumulative distribution function}
	Let $X$ and $Y$ be random variables, then the \emph{joint cumulative probability distribution function} of $X$ and $Y$ is given by
	\begin{alignat}{4}
		F(a, b)&=P\set{X\leq a, Y\leq b}
	\end{alignat}
	for $a,~b\in\mathbb{R}$. 
\end{definition}

\begin{remark}
	Given a joint cumulative distribution function $F:X\times Y\to [0,1]^2$ we may derive the cumulative distribution for $X$ (likewise $Y$) by 
	\begin{alignat}{4}
		F_X(a)&=P\set{X\leq a} \\
			&=P\set{X\leq a, Y<\infty} \\
			&=\lim_{y\to\infty}F(a, y)
	\end{alignat}
\end{remark}

\begin{definition}[Joint Probability Mass Function]\index{probability mass function!joint}\index{joint probability mass function}
	Given discrete random variables $X$ and $Y$ we may define the \emph{joint probability mass function} of $X$ and $Y$ by 
	\begin{alignat}{4}
		p(x, y)&=P\set{X=x, Y=y}
	\end{alignat}
\end{definition}

\begin{proposition}
	Let $p(x, y)$ be a joint probability mass function over discrete random variables $X$ and $Y$ then we may derive the probability mass function for $X$ (and likewise $Y$) by 
	\begin{alignat}{4}
		p_X(x)=\sum_{y:p(x, y)>0} p(x, y)
	\end{alignat}
	that is, we fix $x$ and sum all the probabilities over all $y$. 
\end{proposition}

\begin{definition}[Joint Probability Density Function]\index{probability density function!joint}\index{joint probability density function}
	Let $X$ and $Y$ be continuous random variables, we say that they are \emph{jointly continuous} if there exists a function $f(x, y)$,
	\begin{alignat}{4}
		f:\mathbb{R}^2\to [0,1]
	\end{alignat}
	such that for all $A,~B\subset \mathbb{R}$ the following holds \footnote{Recall that $X:S\to \mathbb{R}$. This tells us the probability that $(X, Y)\in (A, B)$.}
	\begin{alignat}{4}
		f\set{X\in A, Y\in B}=\int_B\int_Af(x, y)dx dy
	\end{alignat}
\end{definition}
\end{spacing}

\begin{proposition}
	Let $X$ and $Y$ be continuous random variables and $f(x, y)$ be a joint probability density function; then we may derive the probability density function for $X$ (likewise $Y$) by 
	\begin{alignat*}{4}
		P\set{X\in A}&=P\set{X\in Y, Y\in \mathbb{R}} \\
			&=\int_\mathbb{R}\int_Af(x, y)dx dy \\
			&=\int_A f_X(x)dx 
	\end{alignat*}
	where
	\begin{alignat}{4}
		f_X(x)&=\int_\mathbb{R}f(x, y)dy
	\end{alignat}
\end{proposition}

\begin{proposition}
	Let $p(x, y)$ be a joint probability mass function of two discrete random variables $X$ and $Y$ respectively; then we may compute the expectation of a function of $(X, Y)$ by 
	\begin{alignat}{4}
		E[g(X, Y)]&=\sum_y\sum_xg(x, y)p(x, y)
	\end{alignat}
\end{proposition}

\begin{proposition}
	Let $f(x, y)$ be a joint probability density function of two continuous random variables $X$ and $Y$ respectively; then we may compute the expectation of a function of $(X, Y)$ by 
	\begin{alignat}{4}
		E[g(X, Y)]&=\int_\mathbb{R}\int_\mathbb{R}g(x, y)f(x, y)dx dy
	\end{alignat}
\end{proposition}

\begin{example}
	Let $X$ and $Y$ be continuous random variables and let $g(X, Y)=X+Y$ then we may compute $E[X+Y]$
	\begin{alignat}{4}
		E[X+Y]&=\int_\mathbb{R}\int_\mathbb{R} (x+y)f(x, y)dxdy \\
			&=\int_\mathbb{R}\int_\mathbb{R} xf(x, y)dx dy + \int_\mathbb{R}\int_\mathbb{R} yf(x, y)dx dy \\
	\end{alignat}
	We treat the integrals separately 
	\begin{alignat}{4}
		\int_\mathbb{R}\int_\mathbb{R} xf(x, y) dx dy &=\int_\mathbb{R}\int_\mathbb{R} x f(x, y) dy dx \label{Eqn:2.3} \\
			&=\int_\mathbb{R} x \int_\mathbb{R} f(x, y) dy dx \\
			&=\int_\mathbb{R} x f_X(x) dx \\
			&= E[X]
	\end{alignat}
	Where Equation \ref{Eqn:2.3} is justified by Fubini's theorem\footnote{This is justified because $f(x, y)$ is non-negative therefore $\left|x f(x, y)\right| = |x|f(x, y)$. When we consider $\int_{-\infty}^\infty |x|f(x, y)dx=\int_{-\infty}^0|x|f(x, y)dx + \int_0^\infty|x|f(x, y)dx=\int_0^\infty xf(x, y)dx$}. 
	
	For the second integral 
	\begin{alignat}{4}
		\int_\mathbb{R}\int_\mathbb{R} yf(x, y) dx dy &=\int_\mathbb{R} y \int_\mathbb{R} f(x, y) dx dy \\
			&= \int_\mathbb{R} y F_Y(y) dy \\
			&= E[Y]
	\end{alignat}
	Therefore
	\begin{alignat}{4}
		E[X+Y] &= E[X] + E[Y]
	\end{alignat}
	
\end{example}

\subsection{Independent Random Variables}

\begin{definition}[Independent Random Variables]\index{random variables!independent}\index{independent random variables}
	Let $X$ and $Y$ be random variables, we say that $X$ and $Y$ are independent if 
	\begin{alignat}{4}
		P\set{X\leq a, Y\leq b}=P\set{X\leq a}P\set{Y\leq b}
	\end{alignat}
	or equivalently in terms of the cumulative distribution functions 
	\begin{alignat}{4}
		F(a, b)=F_X(a)F_Y(b)
	\end{alignat}
	and in terms of the probability mass functions 
	\begin{alignat}{4}
		p(x, y)=p_X(x)p_Y(y)
	\end{alignat}
	and in terms of the probability distribution functions 
	\begin{alignat}{4}
		f(x, y)=f_X(x)f_Y(y)
	\end{alignat}
\end{definition}

\begin{proposition}
	If $X$ and $Y$ are independent random variables then for any functions $h$ and $g$
	\begin{alignat}{4}
		E[g(X)h(Y)]=E[g(X)]E[h(Y)]
	\end{alignat}
\end{proposition}

\subsection{Covariance and Variance of Sums of Random Variables}

\begin{definition}[Covariance]\index{covariance}
	Covariance of two random variables $X$ and $Y$ is defined by 
	\begin{alignat}{4}
		\cov{X, Y}&=E[(X-E[X])(Y-E[Y])] \\
			&=E[XY]-E[X]E[Y]
	\end{alignat}
\end{definition}

\begin{remark}
	The covariance of two random variables $X$ and $Y$ will be $0$ if they are independent. It will be positive if $X>0$ makes it more likely that $Y>0$ (likewise $<0$) and negative covariance implies that $X>0$ makes it more likely that $Y<0$ (as well as the inverse). 
\end{remark}

\begin{proposition}[Properties of Covariance]
	Let $X$, $Y$, $Z$ be random variables and let $c$ be a constant; then
	\begin{enumerate}
		\item $\cov{X, X}=\var{X, X}$
		\item $\cov{X, Y}=\var{Y, X}$
		\item $\cov{cX, Y}=c~\cov{X, Y}$
		\item $\cov{X, Y+Z}=\cov{X, Y}+\cov{X, Z}$
	\end{enumerate}
\end{proposition}

\begin{proposition}[Covariance of Sums of Random Variables]\label{Prop:2.1}
\begin{alignat}{4}
	\cov{\sum_{i=1}^nX_i, \sum_{j=1}^mY_j}&=\sum_{i=1}^n\sum_{j=1^m}\cov{X_i, Y_j}
\end{alignat}
\end{proposition}

\begin{proposition}[Variance of Sums of Random Variables]
	We can use Proposition \ref{Prop:2.1} to show that 
	\begin{alignat}{4}
		\var{\sum_{i=1}^n X_i}&=\sum_{i=1}^n\var{X_i}+2\sum_{i=1}^n\sum_{j<i}\cov{X_i, X_j}
	\end{alignat}
	and if $X_i$ are independent random variables this simplifies to 
	\begin{alignat}{4}
		\var{\sum_{i=1}^n X_i}&=\sum_{i=1}^n\var{X_i}
	\end{alignat}
\end{proposition}

\begin{definition}[Sample Mean]\index{sample mean}\index{mean}
	Let $\set{X_i}_{i=1}^n$ be a sequence of independent identically distributed \emph{(IID)} random variables; then 
	\begin{alignat}{4}
		\widebar{X}=\frac{1}{n}\sum_{i=1}^n X_i
	\end{alignat}
	is a random variable called the \emph{sample mean}
\end{definition}

\begin{remark}
	Let $\set{X_i}_{i=1}^n$ be a sequence of independent identically distributed \emph{(IID)} random variables and let $\widebar{X}$ be the sample mean; then 
	\begin{alignat}{4}
		\widebar{X}:S^n\to \mathbb{R}
	\end{alignat}
	so that $S^n$ is the sample space underlying the random variable. 
\end{remark}

\begin{proposition}
	Suppose that $\set{X_i}$ are IID random variables with expected value $\mu$ and variance $\sigma^2$; then 
	\begin{enumerate}
		\item $E[\widebar{X}] = \mu$ 
		\item $\var{\widebar{X}} = \frac{\sigma^2}{n}$
		\item $\cov{\widebar{X}, X_i-\widebar{X}} = 0$
	\end{enumerate}
\end{proposition}

\subsection{Joint Probability Distribution of Functions of Random Variables}

\begin{theorem}[Joint Probability Distribution of Functions of Random Variables]
	Let $X_1$, $X_2$ be jointly continuous random variables with joint PDF $f(x_1, x_2)$. Suppose $Y_1=g_1(X_1, X_2)$ and $Y_2=g_2(X_1, X_2)$ be random variables for some functions $g_1$, $g_2$. Let $x_1$ and $x_2$ be solvable in terms of $g_1$ and $g_2$ and let the Jacobian $J(x_1, x_2)$ exist at all points; then
	\begin{alignat}{4}
		f_{Y_1, Y_2}(y_1, y_2)=f_{X_1, X_2}(x_1, x_2)\left|J(x_1, x_2)\right|^{-1}
	\end{alignat}
\end{theorem}

\section{Moment Generating Function}

\begin{definition}[Moment Generating Function]\index{moment generating function}
	Let $X$ be a random variable and define $\phi(t)=E\left[e^{tX}\right]$; then $\phi(t)$ is called the moment generating function of $X$. 
\end{definition}

\begin{theorem}
	Let $X$ be a random variable and let $\phi(t)$ be the moment generating function, then the $n^{th}$ moment of $X$ is 
	\begin{alignat}{4}
		\phi^{(n)}(0)
	\end{alignat}
\end{theorem}

\begin{theorem}
	Let $X$ be a random variable then the moment generating function uniquely determines the distribution of $X$. 
\end{theorem}

\subsection{The Joint Distribution of the Sample Mean and the Sample Variance from a Normal Population}

\begin{definition}[$\chi^2$ Random Variable]\index{random variable!$\chi^2$}\index{$\chi^2$ random variable}
	Let $\set{Z_i}$ be a finite set of independent standard normal random variables, then the random variable 
 	\begin{alignat}{4}
 		\chi^2=\sum_{i=1}^n Z_i^2
	\end{alignat}
	is said to be a $\chi^2$ random variable with $n$ degrees of freedom. 
\end{definition}

\begin{proposition}
	If $\set{X_i}_{i=1}^n$ are independent, identically distributed normal random variables with mean $\mu$ and variance $\sigma^2$, then the sample mean $\widebar{X}$ and sample variance $S^2$ are independent. Moreover, $\widebar{X}$ is a normal random variable with mean $\mu$ and variance $\frac{\sigma^2}{n}$. The random variable $\frac{(n-1)S^2}{\sigma^2}$ is a $\chi^2$ random variable with $n-1$ degrees of freedom. 
\end{proposition}

\section{Limit Theorems}

\begin{proposition}[Markov's Inequality]\index{Markov's Inequality}
	If $X$ is a random variable that takes only non-negative values then for any $a>0$ 
	\begin{alignat}{4}
		P\set{X\geq a}\leq \frac{E[X]}{a}
	\end{alignat}
\end{proposition}

\begin{proposition}[Chebyshev's Inequality]\index{Chebyshev's Inequality}
	If $X$ is a random variable with mean $\mu$ and variance $\sigma^2$ then for any $k>0$
	\begin{alignat}{4}
		P\set{|X-\mu|\geq k}\leq \frac{\sigma^2}{k^2}
	\end{alignat}
\end{proposition}

\begin{theorem}[Strong Law of Large Numbers]\index{strong law of large numbers}
Let $\set{X_i}_{i=1}^\infty$ be a sequence of independent random variables having a common distribution and let $E[X_i]=\mu$, then with probability $1$
	\begin{alignat}{4}
		\lim_{n\to\infty}\frac{\sum_{i=1}^n X_i}{n} = \mu
	\end{alignat}
\end{theorem}

\begin{theorem}[Central Limit Theorem]\index{central limit theorem}
	Let $\set{X_i}_{i=1}^n$ be a sequence of independent identically distributed random variables, each with mean $\mu$ and finite variance $\sigma^2$; then the distribution of 
	\begin{alignat}{4}
		\lim_{n\to\infty}\frac{\sum_{i=1}^n X_i - nu}{\sigma\sqrt{n}} = N(0, 1)
	\end{alignat}
	where $N(0, 1)$ denotes the standard normal distribution. 
\end{theorem}	

\section{Stochastic Processes}

\begin{definition}[Stochastic Process]\index{stochastic process}
	Let $\set{X(t)}$ be a collection of random variables indexed by some set $T$ such that for all $t\in T$, $X(t)$ is a random variable. This collection is called a stochastic process.
\end{definition}

\begin{remark}
	Let $\set{X(t)}$ be a stochastic process, we refer to $t$ as the time and $X(t)$ as the state of the process at time $t$. 
\end{remark}

\begin{definition}[State Space]
	The set of all possible values (the range) of $X(t)$ is referred to as the state space. 
\end{definition}	

\section{Problems}

\begin{problem}[\S 2 \# 4]
	Suppose a die is rolled twice. What are the possible values that the following random variables can take on? 
	\begin{enumerate}
		\item The maximum value to appear in the two rolls?
		\item The minimum value to appear in the two rolls? 
		\item The sum of the two rolls? 
		\item The value of the first roll minus the value of the second roll? 
	\end{enumerate}
	\begin{proof}[Solution]
		\
		\begin{enumerate}
			\item $\set{1, \dots, 6}$
			\item $\set{1, \dots, 6}$
			\item $\set{2, \dots, 12}$
			\item $\set{-5, \dots 5}$
		\end{enumerate}
	\end{proof}
\end{problem}

\begin{problem}[\S 2 \# 11]
	A ball is drawn from an urn containing three white and three black balls. After the ball is drawn, it is then replaced and another ball is drawn. This goes on indefinitely. What is the probability that of the first four balls drawn, exactly two are white?
	\begin{proof}[Solution]
		We have four Bernoulli trials with $p=0.5$ of a white ball. We wish to know the probability that there are two successes in four trials from a binomial distribution with parameters $(4, 0.5)$. 
		\begin{alignat}{4}
			p(2)&=\binom{4}{2}(0.5)^2(1-0.5)^2 \\
				&=\frac{4!}{2! 2!}(0.25)(0.25) \\
				&=\frac{4\cdot 3}{2}\frac{1}{4}\frac{1}{4} \\
				&=\frac{6}{1}\frac{1}{16} \\
				&=\frac{3}{8}
		\end{alignat}
	\end{proof}
\end{problem}

\begin{problem}[\S 2 \# 16]
	An airline knows that $5$ percent of the people making a reservations on a certain flight will not show up. Consequently, their policy is to sell $52$ tickets for a flight that can hold only $50$ passengers. What is the probability that there will be a seat available for every passenger who shows up? 
	\begin{proof}[Solution]
		We can consider $52$ Bernoulli random variables with $p=0.95$ percent of success and consider the probability of $51$ or $52$ successes as the probability of an overfull flight and take the remaining probability as being the probability of a non-overfull flight. First we compute the probabilities of $51$ and $52$ people
		\begin{alignat}{4}
			p(51)&=\binom{52}{51}(0.95)^51(0.05)^1 \\
				&=(52)(0.95)^51(0.05) \\
				&\approx 0.190
		\end{alignat}
		Likewise for $52$ passengers 
		\begin{alignat}{4}
			p(52)&=\binom{52}{52}(0.95)^52(0.05)^0 \\
				&=(0.95)^52 \\
				&\approx 0.069
		\end{alignat}
		So the probability of an overfull flight is approximately $0.259$ and the probability of a non-overfull flight is approximately $0.741$
	\end{proof}
\end{problem}

\begin{problem}[\S 2 \# 23]
	A coin having probability $p$ of coming up heads is successively flipped until the $r^{th}$ head appears. Argue that $X$, the number of flips required, with be $n$, with $n \geq r$, with probability 
	\begin{alignat}{4}
		P\set{X=n}=\binom{n-1}{r-1}p^r(1-p)^{n-r}
	\end{alignat}
	\begin{proof}[solution]
		The probability of $r-1$ successes in $n-1$ flips is given by the binomial distribution with parameters $(n-1, r-1)$ 
		\begin{alignat}{4}
			f(r-1; n-1, p)&=\binom{n-1}{r-1}p^{r-1}(1-p)^{(n-1)-(r-1)} \\
				&=\binom{n-1}{r-1}p^{r-1}(1-p)^{(n-r)}
		\end{alignat}
		In order to require precisely $r$ successes in $n$ flips we must have precisely $r-1$ successes in the first $n-1$ flips and then an additional successful flip; so $P\set{X=n}$ is precisely $f(r-1; n-1, p)$ times the probability of an additional success, that is $p$; thus since flips are independent
		\begin{alignat}{4}
			f(r-1; n-1, p)\cdot p&=\binom{n-1}{r-1}p^{r-1}(1-p)^{(n-r)} \cdot p \\
				&=\binom{n-1}{r-1}p^{r}(1-p)^{(n-r)}
		\end{alignat}
	\end{proof}
\end{problem}

\begin{problem}[\S 2 \# 27]
	A fair coin is independently flipped $n$ times, $k$ by $A$ and $n-k$ times by $B$. Show that the probability that $A$ and $B$ flip the same number of heads is equal to the probability that there are a total of $k$ heads. 
	\begin{proof}[Solution]
		First we compute $P\set{X=k}$ where $X$ is a binomial distribution with parameters $(n, p=0.5)$. 
		\begin{alignat}{4}
			f(k; n, p)&=\binom{n}{k}\paren{\frac{1}{2}}^{k}\paren{\frac{1}{2}}^{n-k} \\
				&=\binom{n}{k}\paren{\frac{1}{2}}^n
		\end{alignat}
	\end{proof}
	Next we compute the probability that we have the same number of heads for person $A$ and person $B$; where the probability of $i$ heads for each is 
	\begin{alignat}{4}
		P\set{A=i}&=\binom{k}{i}\paren{\frac{1}{2}}^i\paren{\frac{1}{2}}^{(k-i)} \\
		P\set{B=i}&=\binom{n-k}{i}\paren{\frac{1}{2}}^{i}\paren{\frac{1}{2}}^{n-k-i}
	\end{alignat}
	Assume WLOG that $k<n-k$ and note that these probabilities are independent then we compute 
	\begin{alignat}{4}
		P\set{A=B}&=\sum_{i=0}^nP\set{A=i}\cdot P\set{B=i} \\
			&=\sum_{i=0}^k\binom{k}{i}\paren{\frac{1}{2}}^i\paren{\frac{1}{2}}^{(k-i)}\binom{n-k}{i}\paren{\frac{1}{2}}^{i}\paren{\frac{1}{2}}^{n-k-i} \\
			&=\sum_{i=0}^k\binom{k}{i}\binom{n-k}{i}\paren{\frac{1}{2}}^n \label{Eqn:2.27.1} \\
			&=\paren{\frac{1}{2}}^n\sum_{i=0}^k\binom{k}{i}\binom{n-k}{i} \\ 
			&=\paren{\frac{1}{2}}^n\sum_{i=0}^k\binom{k}{k-i}\binom{n-k}{i} \\
			&=\binom{n}{k}\paren{\frac{1}{2}}^n \label{Eqn:2.27.2}
	\end{alignat}
	Where equation \ref{Eqn:2.27.1} follows from algebra on the exponents and \ref{Eqn:2.27.2} follows from Vandermode's identity. 
\end{problem}

\begin{problem}[\S 2 \# 38]
	If the density function of $X$ equal 
	\begin{alignat}{4}
		f(x)=\left\{\begin{array}{ll}
			ce^{-2x},&0<x<\infty \\
			0,&x<0
			\end{array}\right.
	\end{alignat}
	find $c$. What is $P\set{X>2}$? 
	\begin{proof}[Solution]
		We know that $\int f(x)dx = 1$ so that 
		\begin{alignat}{4}
			\int_\mathbb{R}f(x)dx &= \int_\mathbb{R} ce^{-2x} dx \\
				&=\left.-\frac{1}{2}ce^{-2x}\right|_{x=0}^\infty
		\end{alignat}
		At $\infty$ this is 0 and at $x=0$ this is 
		\begin{alignat}{4}
			-\frac{1}{2}\frac{c}{1}&=-\frac{c}{2}
		\end{alignat}
		so we require that $\frac{c}{2}=1\implies c=2$. 
		
		Now we compute
		\begin{alignat}{4}
			P\set{X>2}=\int_2^\infty 2e^{-2x}dx \\
				&=\left.-\frac{1}{2}(2)e^{-2x}\right|_{x=2}^\infty \\
				&=e^{-2(2)} \\
				&=e^{-4}
		\end{alignat}
	\end{proof}
\end{problem}

\begin{problem}[\S 2 \# 47]
	Consider three trials, each of which is either a success or failure. Let $X$ denote the number of successes. Suppose that $E[X]=1.8$
	\begin{enumerate}
		\item What is the largest possible value for $P\set{X=3}$
		\item What is the smallest possible value for $P\set{X=3}$
	\end{enumerate}
	In both cases, construct a probability scenario that results in $P\set{X=3}$ having the desired value. 
	\begin{proof}[Solution]
		By Markov's inequality we find that 
		\begin{alignat}{4}
			P\set{X=3}&=P\set{X\geq 3} \\
				&\leq \frac{E[X]}{3} \\
				&=\frac{1.8}{3} \\
				&=0.6
		\end{alignat}
		suppose then that $X_1$, $X_2$, $X_3$ are the three trials and further that $X_1=X_2=X_3$, then we can conclude that 
		\begin{alignat}{4}
			1.8&=E[X] \\
				&=P\set{X_1=1}+P\set{X_2=1}+P\set{X+3=1} \\
				&=3\cdot P\set{X_1=1}
		\end{alignat}
		and thus $P\set{X_i=1}=0.6$ is one scenario. 
		
		For the second part we note that independence is not required, let $Y$ be the uniform distribution on $(0, 1)$ and define $X_1=1$ if $U\leq 0.6$, $X_2=1$ if $U\geq 0.4$ and $X_3=1$ if $U\leq 0.3$ or $U\geq 0.7$. Let $U$ be given, then we can see there's no value of $U$ such that $X=3$ but that the expected value of each $X_i$ is $0.6$. 

	\end{proof}
\end{problem}

\begin{problem}[\S 2 \# 49]
	Prove that $E\left[X^2\right]\geq \left(E[X]\right)^2$
	\begin{proof}[Solution]
		Notice that expectation forms a inner product space and so we may write 
		\begin{alignat}{4}
			E[X^2]&=<X, X> \\
				&=<X, X>\cdot<1, 1> \\
				&\geq <X, 1>^2 \\
				&=E[X]^2
		\end{alignat}
		by Cauchy Schwarz. The only way equality holds is if $X$ and $1$ differ by a constant; that is, $X$ is constant and therefore the variance is $0$. 
	\end{proof}
\end{problem}

\begin{problem}[\S 2 \# 70]
	Show that 
	\begin{alignat}{4}
		\lim_{n\to\infty}e^{-n}\sum_{k=0}^n\frac{n^k}{k!}=\frac{1}{2}
	\end{alignat}
	\begin{proof}[Solution]
		Let $X_k$ be lambda random variables with $\lambda=1=\mu=\sigma$. Note that 
		\begin{alignat}{4}
			P\set{X_k=i}=e^{-\lambda} \frac{\lambda^i}{i!}
		\end{alignat}
		Now consider $X=\sum_{k=1}^n X_k$, this is a lambda distribution with $\lambda = n$, so 
		\begin{alignat}{4}
			P\set{X=i}=e^{-n}\frac{n^i}{i!}
		\end{alignat}
		now consider $P\set{X\leq n}$,
		\begin{alignat}{4}
			P\set{X\leq n}&=\sum_{k=1}^nP\set{X=k} \\
				&=\sum_{k=1}^ne^{-n}\frac{n^k}{k!} \\
				&=e^{-n}\sum_{k=1}^n\frac{n^k}{k!}
		\end{alignat}
		
		But notice that
		\begin{alignat}{4}
			P\set{X\leq n}&=P\set{\sum_{k=1}^n X_k \leq n} \\
				&=\set{\sum_{k=1}^nX_k -n \leq 0} \\
				&=\set{\frac{\sum_{k=1}^n X_k -n\mu}{\sigma\sqrt{n}}\leq 0} \\
				&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 e^{\frac{-x^2}{2}} dx
		\end{alignat}
		by the central limit theorem. This is a cumulative normal distribution with mean 0 upto 0 and by symmetry of the distribution this is equal to $\frac{1}{2}$
	\end{proof}
\end{problem}

\chapter{Conditional Probability and Conditional Expectation}

\section{The Discrete Case}

\begin{remark}
	Recall that if $E$ and $F$ are events and $P(F)>0$ then the conditional probability of $E$ given $F$ is 
	\begin{alignat}{4}
		P(E\mid F)&=\frac{P(EF)}{P(F)}
	\end{alignat}
\end{remark}


\clearpage
\addcontentsline{toc}{chapter}{Index}
\printindex

\end{document}  